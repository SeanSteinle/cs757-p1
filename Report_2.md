**Objective**

The objective of this work was to develop and evaluate generative models for audio synthesis, specifically focusing on Variational Autoencoders (VAE) and their variants, including Hierarchical VAE (HVAE) and Vector Quantized VAE (VQVAE). The goal was to efficiently encode temporal structures into a low-dimensional latent space while preserving the acoustic properties necessary for realistic audio generation. The primary challenge was to ensure that the models captured meaningful latent representations without experiencing posterior collapse or generating incoherent outputs. Additionally, processing long sequences posed computational challenges that required careful architectural and data processing decisions to balance expressiveness, efficiency, and training feasibility.

**Modeling approach**

Initially, spectrogram representations were explored alongside raw waveforms to assess their effectiveness in capturing audio features. The spectrogram-based approach involved extracting Mel spectrograms, normalizing them, and mapping them to a latent space for reconstruction. However, the conversion process from audio to spectrogram and back introduced artifacts and loss of information, which affected the quality of the reconstructed signals. To mitigate this, we experimented with different spectrogram resolutions, particularly increasing the number of Mel filters to 256, which yielded better results compared to 128 but was still limited by computational constraints.

For the raw signal approach, the audio waveforms were normalized to prevent numerical instability and segmented into overlapping chunks to manage memory constraints. The encoder extracted temporal features using strided 1D convolutional layers, reducing dimensionality while preserving key signal characteristics. A latent sampling mechanism was implemented with KL divergence regularization to ensure a well-structured latent space. The decoder mirrored the encoder, applying transposed convolutions and learned upsampling strategies to reconstruct the waveform without excessive cropping, which could remove valuable information. The loss function was a combination of mean squared error for reconstruction quality and a KL divergence penalty with β-VAE constraints to balance latent space regularization and reconstruction fidelity.

**Challenges and solutions in VAE and HVAE**

The spectrogram-based VAE initially trained smoothly, but the reconstruction quality was suboptimal due to the inherent noise in converting between the spectrogram and waveform domains. Increasing the latent dimensionality and adjusting the learning rate provided some improvements, but further refinement of the model size was necessary to capture more complex structures. The HVAE was introduced to extract hierarchical latent features with the hypothesis that a two-level latent space would improve the representation power of the model. However, results were inconsistent, and in some cases, reconstruction performance degraded. This was likely due to difficulties in selecting appropriate hyperparameters to balance information propagation between hierarchical latent spaces. Ensuring that the higher-level latents meaningfully influenced lower-level representations required refining the hierarchical prior, adjusting the KL divergence weighting, and explicitly defining relationships between the two latent levels in the decoder.

For the raw waveform-based VAE, the reconstruction quality was initially poor, leading to modifications in the network architecture. Using global average pooling instead of flattening in the encoder significantly reduced the number of parameters while maintaining critical temporal information. The model also exhibited high variance in latent encodings, making training unstable. Batch normalization was introduced in the latent space to address this issue, leading to improved training stability.

**Challenges and solutions in VQVAE**

The VQVAE posed additional challenges, particularly in codebook utilization and commitment loss tuning. A key issue was mode collapse, where only a limited number of codebook embeddings were used, reducing the model’s capacity for diverse generation. This was mitigated by modifying the codebook update mechanism and adding a loss term to encourage a more even distribution of embedding usage. Balancing the commitment loss was also critical because setting it too high caused embeddings to be overly constrained, leading to poor generalization, while setting it too low resulted in unstable training dynamics. To address this, the β parameter in the loss function was adjusted dynamically based on the variance of the latent encodings, improving convergence.

Architecturally, discrepancies in output dimensions between the encoder and decoder necessitated restructuring the upsampling path to ensure smooth dimensionality restoration. This adjustment reduced the need for explicit cropping, which could discard essential features. Despite multiple adjustments to network parameters and settings, the VQ loss remained highly unstable, even when reconstruction loss improved. This instability likely stemmed from the discrete nature of vector quantization, where embeddings struggle to converge due to insufficient gradient flow. Further tuning of the training dynamics and alternative methods for updating the codebook could be explored to address this issue.

**Handling long sequences and data processing**

A significant challenge across all models was processing long sequences efficiently. Given memory constraints, full-length waveforms could not be directly used, necessitating chunking strategies to preserve meaningful temporal dependencies. Instead of simple truncation, overlapping segments were extracted to maintain contextual continuity. A sequence length of 22050 samples (corresponding to one second of audio at a 22.05 kHz sample rate) was selected to balance signal fidelity and computational feasibility. Additionally, waveform normalization was applied before training.

**Evaluation and observations**

The models were evaluated using both quantitative and qualitative metrics. MSE and cosine similarity provided numerical assessments of reconstruction quality, while subjective listening tests revealed that generations were often dominated by noise. Sampling directly from the latent prior produced incoherent outputs, prompting a shift toward interpolating real encodings to explore the latent space more effectively. The VAE exhibited limited fidelity in preserving fine audio details, while the HVAE provided marginal improvements but suffered from mode averaging, which blurred intricate features. The VQVAE showed potential in preserving discrete structures but required additional refinements to ensure better embedding utilization.

**Conclusion**

This work explored generative audio synthesis using VAE, HVAE, and VQVAE models, each presenting unique challenges in capturing and reconstructing audio features. While the spectrogram-based models offered structured feature extraction, they introduced conversion noise that limited reconstruction quality. The raw waveform models were more challenging to train but provided a more direct representation of the audio signal. The hierarchical and vector quantization approaches introduced complexity in latent space structuring, requiring careful balancing of information flow and loss weighting. Future improvements could involve alternative priors, adversarial training, or attention mechanisms to enhance generation quality and stability.
