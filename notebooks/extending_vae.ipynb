{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee34f5b-d91c-4abf-a935-2982a80321db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Extending VAEs!\n",
    "\n",
    "## Table of Contents\n",
    "1. [VQ-VAE](#VQ-VAE)\n",
    "2. [Hierarchical VAE](#Hierarchical-VAE)\n",
    "3. [VAE with Simple Waveform](#VAE-with-Simple-Waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d17401-4f13-416e-acd9-c8c0e6b78e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 14:20:55.382592: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 14:20:55.408754: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 14:20:55.632495: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 14:20:55.832313: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742235656.000878   28649 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742235656.051143   28649 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-17 14:20:56.448983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import librosa, os #audio processing and file system parsing\n",
    "import librosa.display\n",
    "import numpy as np #math library\n",
    "import tensorflow as tf #for model building\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Reshape, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import pandas as pd #for data analysis / prep\n",
    "import IPython.display as ipd #for sound output\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import soundfile as sf\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d96be-8fce-40ef-81bb-3e6fdfa040b3",
   "metadata": {},
   "source": [
    "Before we get into it, let's load some of the core functions and data from our previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2beaf40e-7b10-448f-a8dc-4314218b88b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28649/298290904.py:12: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  original_audio, sampling_rate = librosa.load(os.path.join(audio_dir, file), sr=22050)\n",
      "/home/seansteinle/.pyenv/versions/3.10.16/envs/cs757-p1/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file jazz.00054.wav due to error: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gt_audio</th>\n",
       "      <th>gt_spectrogram</th>\n",
       "      <th>min_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jazz.00080.wav</td>\n",
       "      <td>[-0.010284424, -0.018707275, -0.014312744, -0....</td>\n",
       "      <td>[[0.5306781, 0.5076559, 0.30387545, 0.19764082...</td>\n",
       "      <td>(-72.97773, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jazz.00098.wav</td>\n",
       "      <td>[-0.031066895, -0.05078125, -0.04537964, -0.04...</td>\n",
       "      <td>[[0.5447164, 0.46910134, 0.44315043, 0.4229424...</td>\n",
       "      <td>(-80.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jazz.00001.wav</td>\n",
       "      <td>[0.0024108887, 0.005493164, 0.008666992, 0.011...</td>\n",
       "      <td>[[0.5821481, 0.5379497, 0.44007826, 0.49471942...</td>\n",
       "      <td>(-80.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jazz.00069.wav</td>\n",
       "      <td>[-0.029541016, -0.07070923, -0.10971069, -0.13...</td>\n",
       "      <td>[[0.64533484, 0.57680035, 0.38201648, 0.383343...</td>\n",
       "      <td>(-80.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jazz.00050.wav</td>\n",
       "      <td>[-0.13595581, -0.09390259, -0.06210327, -0.216...</td>\n",
       "      <td>[[0.71702677, 0.64992064, 0.6152149, 0.6068060...</td>\n",
       "      <td>(-76.36797, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename                                           gt_audio  \\\n",
       "0  jazz.00080.wav  [-0.010284424, -0.018707275, -0.014312744, -0....   \n",
       "1  jazz.00098.wav  [-0.031066895, -0.05078125, -0.04537964, -0.04...   \n",
       "2  jazz.00001.wav  [0.0024108887, 0.005493164, 0.008666992, 0.011...   \n",
       "3  jazz.00069.wav  [-0.029541016, -0.07070923, -0.10971069, -0.13...   \n",
       "4  jazz.00050.wav  [-0.13595581, -0.09390259, -0.06210327, -0.216...   \n",
       "\n",
       "                                      gt_spectrogram           min_max  \n",
       "0  [[0.5306781, 0.5076559, 0.30387545, 0.19764082...  (-72.97773, 0.0)  \n",
       "1  [[0.5447164, 0.46910134, 0.44315043, 0.4229424...      (-80.0, 0.0)  \n",
       "2  [[0.5821481, 0.5379497, 0.44007826, 0.49471942...      (-80.0, 0.0)  \n",
       "3  [[0.64533484, 0.57680035, 0.38201648, 0.383343...      (-80.0, 0.0)  \n",
       "4  [[0.71702677, 0.64992064, 0.6152149, 0.6068060...  (-76.36797, 0.0)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define hop_length (adjust as needed)\n",
    "HOP_LENGTH = 512\n",
    "TIME_FRAMES = 1280  # Instead of 1290\n",
    "N_MELS = 256\n",
    "def load_data(audio_dir, n_mels=N_MELS, time_frames=TIME_FRAMES):\n",
    "    \"\"\"Processes a directory of audio files into a DataFrame containing the spectrograms and original waveforms.\"\"\"\n",
    "    audio_files = os.listdir(audio_dir)\n",
    "    audio_data = {'filename': [], 'gt_audio': [], 'gt_spectrogram': [], 'min_max': []}\n",
    "\n",
    "    for file in audio_files:\n",
    "        try:\n",
    "            original_audio, sampling_rate = librosa.load(os.path.join(audio_dir, file), sr=22050)  \n",
    "\n",
    "            mel_spectrogram = librosa.feature.melspectrogram(y=original_audio, sr=sampling_rate, n_mels=n_mels, hop_length=HOP_LENGTH)\n",
    "            S_dB = librosa.power_to_db(mel_spectrogram, ref=np.max)  # Normalize based on max power\n",
    "            \n",
    "            # Save min/max values for proper denormalization later\n",
    "            S_dB_min = np.min(S_dB)\n",
    "            S_dB_max = np.max(S_dB)\n",
    "            \n",
    "            # Normalize to [0,1]\n",
    "            S_dB = (S_dB - S_dB_min) / (S_dB_max - S_dB_min)\n",
    "\n",
    "            # **Truncate or pad spectrogram to $time frames**\n",
    "            if S_dB.shape[1] < TIME_FRAMES:\n",
    "                pad_width = TIME_FRAMES - S_dB.shape[1]\n",
    "                S_dB = np.pad(S_dB, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "            elif S_dB.shape[1] > TIME_FRAMES:\n",
    "                S_dB = S_dB[:, :TIME_FRAMES]\n",
    "                \n",
    "            # Store data\n",
    "            audio_data['filename'].append(file)\n",
    "            audio_data['gt_audio'].append(original_audio)\n",
    "            audio_data['gt_spectrogram'].append(S_dB)\n",
    "            audio_data['min_max'].append((S_dB_min, S_dB_max))  # Save for denormalization\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {file} due to error: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(audio_data)\n",
    "\n",
    "def reconstruct_audio(spectrogram, sr=22050, n_mels=N_MELS, min_max=None):\n",
    "    \"\"\"Reconstructs audio from a normalized Mel spectrogram. Utilizes parameters from normalization to reverse process!\"\"\"\n",
    "    spectrogram = np.array(spectrogram)\n",
    "    S_dB_min, S_dB_max = min_max\n",
    "    S_dB = spectrogram * (S_dB_max - S_dB_min) + S_dB_min  # Reverse normalization\n",
    "\n",
    "    # Convert back to power spectrogram (ensure this transformation is correct)\n",
    "    S = librosa.db_to_power(S_dB)  \n",
    "\n",
    "    # Convert Mel spectrogram back to waveform\n",
    "    reconstructed_audio = librosa.feature.inverse.mel_to_audio(S, sr=sr, hop_length=HOP_LENGTH)  \n",
    "    return reconstructed_audio\n",
    "\n",
    "\n",
    "jazz_df = load_data(\"../data/kaggle/genres_original/jazz/\")\n",
    "jazz_songs = np.array(jazz_df['gt_spectrogram'].tolist()) #transform song shapes\n",
    "jazz_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f65b9-a510-4df7-88fb-b7991ac8125c",
   "metadata": {},
   "source": [
    "## VQ VAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1c111-1bf6-41e3-9bb7-e70616350431",
   "metadata": {},
   "source": [
    "Here is how we extend the standard VAE by introducing the discrete latent variables (instead of continous ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13bd537d-d6aa-4295-9995-68e8321513e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Layer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization, LayerNormalization\n",
    "\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d73abd-9117-439e-861c-312f88a5c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- VECTOR QUANTIZER ---- #\n",
    "class VectorQuantizer(Layer):\n",
    "    \"\"\" Implements the vector quantization layer, which maps continuous latent representations\n",
    "        to discrete embeddings from a learned codebook. This layer enables discrete latent learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.beta = beta  # Commitment cost to ensure latent representations remain close to embeddings\n",
    "\n",
    "        # Initialize the embedding codebook with uniform distribution\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\" Performs vector quantization by mapping continuous inputs to their closest embeddings. \"\"\"\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])  # Flatten input to match embedding dimension\n",
    "\n",
    "        # Compute nearest embedding index\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)  # Convert to one-hot representation\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)  # Retrieve nearest embeddings\n",
    "\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Compute vector quantization loss (encouraging commitment to embeddings)\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator: Pass gradients through quantized values\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        \"\"\" Computes distances between input features and embedding vectors to find closest match. \"\"\"\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Identify nearest embedding indices based on minimal distance\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices\n",
    "\n",
    "# ---- MODEL CREATION ---- #\n",
    "def get_encoder(latent_dim):\n",
    "    \"\"\" Defines the encoder network that compresses the input spectrogram into a lower-dimensional latent space. \"\"\"\n",
    "    encoder_inputs = keras.Input(shape=(256, 1280, 1))  \n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")(encoder_inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = layers.Conv2D(256, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    encoder_outputs = layers.Conv2D(latent_dim, (1, 1), padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "def get_decoder(latent_dim):\n",
    "    \"\"\"Defines a decoder that mirrors the encoder's upsampling path correctly.\"\"\"\n",
    "    latent_inputs = keras.Input(shape=get_encoder(latent_dim).output.shape[1:])\n",
    "\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")(latent_inputs)  \n",
    "    x = layers.Conv2DTranspose(128, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")(x) \n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")(x)  \n",
    "\n",
    "    # Final layer to reconstruct input spectrogram\n",
    "    x = layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x) \n",
    "\n",
    "    return keras.Model(latent_inputs, x, name=\"decoder\")\n",
    "\n",
    "def get_vqvae(latent_dim, num_embeddings, beta):\n",
    "    \"\"\" Assembles the VQ-VAE model by integrating the encoder, vector quantizer, and decoder. \"\"\"\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, beta, name=\"vector_quantizer\")\n",
    "    encoder = get_encoder(latent_dim)\n",
    "    decoder = get_decoder(latent_dim)\n",
    "    \n",
    "    inputs = keras.Input(shape=(256, 1280, 1))  # Spectrogram input\n",
    "    encoder_outputs = encoder(inputs)  # Compress input into latent space\n",
    "    quantized_latents = vq_layer(encoder_outputs)  # Discretize latent representations\n",
    "    reconstructions = decoder(quantized_latents)  # Reconstruct spectrogram\n",
    "\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
    "\n",
    "class VQVAETrainer(keras.models.Model):\n",
    "    \"\"\" Custom training loop for VQ-VAE to compute losses and update embeddings. \"\"\"\n",
    "    def __init__(self, train_variance, latent_dim, num_embeddings, beta, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_variance = train_variance  # Variance of training data (used for normalization)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings, beta)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vqvae_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\" Returns tracked metrics for training evaluation. \"\"\"\n",
    "        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.vq_loss_tracker]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        \"\"\" Performs a single training step, including forward pass, loss computation, and optimization. \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = tf.expand_dims(x, axis=-1)  \n",
    "            reconstructions = self.vqvae(x)  # Forward pass through the VQ-VAE model\n",
    "\n",
    "            # Compute reconstruction loss (MSE) normalized by training variance\n",
    "            reconstruction_loss = tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)  # Total loss includes VQ loss\n",
    "\n",
    "        # Compute gradients and update model parameters\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Update tracked loss metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad58e17-6c82-4063-827d-190a3114109b",
   "metadata": {},
   "source": [
    "###  Balancing latent dimension, num_embeddings, and commitment loss is challenging. Fine-tuning these hyperparameters is essential for stable and expressive representations.\n",
    "\n",
    "Higher commitment loss ==> Stronger pull towards embeddings, increasing VQ loss but improving discrete representation\n",
    "\n",
    "Lower commitment loss ==> Weaker pull, reducing VQ loss, but risking unstable training\n",
    "\n",
    "Low latent dim / shallow model ==> Insufficient capacity, failing to capture complex structures\n",
    "\n",
    "High latent dim ==> Better representation, but risks overfitting or codebook collapse if num_embeddings is too low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce701c16-ab39-47d5-8e2a-42b9942e2e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 14:21:14.223906: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vq_vae\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vq_vae\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">403,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vector_quantizer                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">VectorQuantizer</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">664,577</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1280\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │       \u001b[38;5;34m403,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vector_quantizer                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mVectorQuantizer\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1280\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │       \u001b[38;5;34m664,577\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,068,033</span> (4.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,068,033\u001b[0m (4.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,068,033</span> (4.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,068,033\u001b[0m (4.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# ---- TRAINING PARAMETERS ---- #\n",
    "latent_dim = 128  # Controls the size of the compressed latent space. A higher value captures more detail.\n",
    "num_embeddings = 32  # Number of unique codebook vectors available for quantization.\n",
    "beta = 0.75\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "# visualizing the model\n",
    "vqvae_model = get_vqvae(latent_dim, num_embeddings, beta)\n",
    "vqvae_model.summary()\n",
    "\n",
    "# ---- MODEL TRAINING ---- #\n",
    "# Compute variance of training data to normalize loss calculations.\n",
    "train_variance = tf.math.reduce_variance(jazz_songs)\n",
    "\n",
    "vqvae_trainer = VQVAETrainer(train_variance, latent_dim=latent_dim, num_embeddings=num_embeddings, beta=beta)\n",
    "vqvae_trainer.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005))\n",
    "\n",
    "initial_embeddings = vqvae_model.get_layer(\"vector_quantizer\").embeddings.numpy().copy()  # Store initial embeddings\n",
    "\n",
    "# Training:\n",
    "history = vqvae_trainer.fit(jazz_songs, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# ---- VISUALIZATION ---- #\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss During Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "pred_spectrograms_vqvae = vqvae_model.predict(jazz_songs)\n",
    "jazz_df['pred_spectrogram_vqvae'] = pd.Series(pred_spectrograms_vqvae.tolist())\n",
    "jazz_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfcf4a3-f269-42f3-ba69-d78924b009dd",
   "metadata": {},
   "source": [
    "### Another settings for the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b943e1-ca3e-42d6-8f94-762839682e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TRAINING PARAMETERS ---- #\n",
    "latent_dim = 256  # Controls the size of the compressed latent space. A higher value captures more detail.\n",
    "num_embeddings = 8  # Number of unique codebook vectors available for quantization.\n",
    "beta = 0.5\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "# visualizing the model\n",
    "vqvae_model = get_vqvae(latent_dim, num_embeddings, beta)\n",
    "vqvae_model.summary()\n",
    "\n",
    "# ---- MODEL TRAINING ---- #\n",
    "# Compute variance of training data to normalize loss calculations.\n",
    "train_variance = tf.math.reduce_variance(jazz_songs)\n",
    "\n",
    "vqvae_trainer = VQVAETrainer(train_variance, latent_dim=latent_dim, num_embeddings=num_embeddings, beta=beta)\n",
    "vqvae_trainer.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005))\n",
    "\n",
    "initial_embeddings = vqvae_model.get_layer(\"vector_quantizer\").embeddings.numpy().copy()  # Store initial embeddings\n",
    "\n",
    "# Training:\n",
    "history = vqvae_trainer.fit(jazz_songs, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# ---- VISUALIZATION ---- #\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss During Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "pred_spectrograms_vqvae = vqvae_model.predict(jazz_songs)\n",
    "jazz_df['pred_spectrogram_vqvae'] = pd.Series(pred_spectrograms_vqvae.tolist())\n",
    "jazz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cd078-30f2-4af8-a689-83661c07d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- EVALUATE RECONSTRUCTION QUALITY ---- #\n",
    "\n",
    "# Higher values indicate better reconstructions.\n",
    "jazz_df['cosine_similarity_vqvae'] = compute_cosine_similarity(jazz_df, 'gt_spectrogram', 'pred_spectrogram_vqvae')\n",
    "\n",
    "sorted_similarities_vqvae = jazz_df['cosine_similarity_vqvae'].sort_values()\n",
    "worst_idx_vqvae = jazz_df['cosine_similarity_vqvae'].idxmin()  \n",
    "best_idx_vqvae = jazz_df['cosine_similarity_vqvae'].idxmax()   \n",
    "\n",
    "print(f\"Worst VQ-VAE Reconstruction Index: {worst_idx_vqvae}\")\n",
    "print(f\"Best VQ-VAE Reconstruction Index: {best_idx_vqvae}\")\n",
    "\n",
    "print(jazz_df['cosine_similarity_vqvae'].describe())  # Expect higher values if reconstruction improves\n",
    "\n",
    "# ---- AUDIO COMPARISON ---- #\n",
    "def compare_song_audios_vqvae_and_save(jazz_df, sample_n, output_filename=None):\n",
    "    try:\n",
    "        pred_audio_vqvae = reconstruct_audio(\n",
    "            jazz_df.iloc[sample_n]['pred_spectrogram_vqvae'],\n",
    "            sr=22050, n_mels=128, min_max=jazz_df.iloc[sample_n]['min_max']\n",
    "        )\n",
    "\n",
    "        original_audio = jazz_df.iloc[sample_n]['gt_audio']\n",
    "\n",
    "        # Normalize for 16-bit PCM (-32768 to 32767)\n",
    "        pred_audio_int16 = np.int16(pred_audio_vqvae / np.max(np.abs(pred_audio_vqvae)) * 32767)\n",
    "\n",
    "        # Save if output filename is provided\n",
    "        if output_filename is not None:\n",
    "            try:\n",
    "                with sf.SoundFile(output_filename, mode='w', samplerate=22050, channels=1, subtype='PCM_16') as file:\n",
    "                    file.write(pred_audio_int16)\n",
    "                print(f\"Saved audio to: {output_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving {output_filename}: {e}\")\n",
    "\n",
    "        # Normalize for IPython Audio playback (IPython requires float32 in [-1,1])\n",
    "        pred_audio_ipd = pred_audio_vqvae / np.max(np.abs(pred_audio_vqvae))\n",
    "\n",
    "        return ipd.Audio(original_audio, rate=22050), ipd.Audio(pred_audio_ipd, rate=22050)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "real_vqvae, pred_vqvae = compare_song_audios_vqvae_and_save(jazz_df, best_idx_vqvae, output_filename=\"output/VQ-VAE_best.wav\")\n",
    "print(\"Best reconstruction\")\n",
    "display(real_vqvae)\n",
    "display(pred_vqvae)\n",
    "\n",
    "real_vqvae, pred_vqvae = compare_song_audios_vqvae_and_save(jazz_df, worst_idx_vqvae, output_filename=\"output/VQ-VAE_worst.wav\")\n",
    "print(\"worst reconstruction\")\n",
    "display(real_vqvae)\n",
    "display(pred_vqvae)\n",
    "\n",
    "# ---- VISUALIZING SPECTROGRAM RECONSTRUCTIONS ---- #\n",
    "def plot_vqvae_spectrogram_comparison(jazz_df: pd.DataFrame, index: int):\n",
    "    \"\"\"\n",
    "    Plots the original and VQ-VAE reconstructed spectrogram side by side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "    # Original spectrogram\n",
    "    axes[0].imshow(jazz_df.iloc[index]['gt_spectrogram'], origin='lower', aspect='auto')\n",
    "    axes[0].set_title(\"Original Spectrogram\")\n",
    "\n",
    "    # VQ-VAE reconstructed spectrogram\n",
    "    axes[1].imshow(jazz_df.iloc[index]['pred_spectrogram_vqvae'], origin='lower', aspect='auto')\n",
    "    axes[1].set_title(\"VQ-VAE Reconstructed Spectrogram\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot and compare the worst and best reconstructions\n",
    "plot_vqvae_spectrogram_comparison(jazz_df, worst_idx_vqvae)\n",
    "plot_vqvae_spectrogram_comparison(jazz_df, best_idx_vqvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dde28-1f6b-4b5c-b0f5-b4bb50dc0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CHECK EMBEDDING CHANGES OVER TRAINING ---- #\n",
    "\n",
    "vq_layer = vqvae_model.get_layer(\"vector_quantizer\")  # Ensure this matches the layer name\n",
    "trained_embeddings = vq_layer.embeddings.numpy()\n",
    "\n",
    "embedding_change = np.abs(trained_embeddings - initial_embeddings)\n",
    "\n",
    "print(\"Initial vs. Trained Embeddings):\")\n",
    "for i in range(5):\n",
    "    print(f\"Embedding {i}:\")\n",
    "    print(f\"  Initial: {initial_embeddings[i]}\") \n",
    "    print(f\"  Trained: {trained_embeddings[i]}\") \n",
    "    print(f\"  Change:  {embedding_change[i]}\")  \n",
    "\n",
    "# ---- CHECK HOW WELL EMBEDDINGS ARE UTILIZED ---- #\n",
    "encoder = vqvae_model.get_layer(\"encoder\") \n",
    "z_e = encoder.predict(jazz_songs)\n",
    "try: \n",
    "    # Reshape encoder output to (batch_size * num_patches, latent_dim)\n",
    "    z_e = z_e.reshape(-1, z_e.shape[-1]) \n",
    "    \n",
    "    # Compute encoding indices \n",
    "    encoding_indices = np.argmin(\n",
    "        np.sum(z_e**2, axis=1, keepdims=True)  \n",
    "        - 2 * np.matmul(z_e, trained_embeddings.T)\n",
    "        + np.sum(trained_embeddings**2, axis=0),  \n",
    "        axis=1 \n",
    "    )\n",
    "    \n",
    "    # Count the number of unique embeddings that were actually used during training\n",
    "    unique_embeddings_used = len(np.unique(encoding_indices))\n",
    "    print(f\"Unique embeddings used: {unique_embeddings_used} / {num_embeddings}\")\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(encoding_indices, bins=num_embeddings, range=(0, num_embeddings - 1))\n",
    "    plt.xlabel(\"Embedding Index\")\n",
    "    plt.ylabel(\"Usage Count\")\n",
    "    plt.title(\"Used Embedding Indices\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(z_e.flatten(), bins=100)\n",
    "    plt.title(\"Latent Space Activations\")\n",
    "    plt.xlabel(\"Activation Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c93f6-f8a2-4576-a030-522802a84dc7",
   "metadata": {},
   "source": [
    "The VQ-VAE training was unstable. The VQ loss kept increasing, even when the reconstruction loss improved. Experimenting with different beta values, batch sizes, and learning rates did not help. The main issue was poor embedding updates I think, likely due to batch normalization interfering with quantization or wrong implementation. Even after adjustments, the codebook usage remained low, meaning many embeddings were never assigned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3cf49-0b12-4e93-bb1c-720c04596ff7",
   "metadata": {},
   "source": [
    "### So lets try another approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d943c-0cb4-4b93-a916-fc738bf463d2",
   "metadata": {},
   "source": [
    "Due to these challenges, we decided to use Hierarchical VAE (HVAE) instead. \n",
    "HVAE keeps the latent space continuous while adding an extra latent layer. This allows it to capture high-level and low-level features separately. HVAE is also easier to train than VQ-VAE since it does not rely on discrete representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866e1ed-8ad2-4e28-8c10-1d037fe9d7e8",
   "metadata": {},
   "source": [
    "# Hierarchical VAE (HVAE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df5f03-8afd-4f4a-8b0b-1d015cc27e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hvae():\n",
    "    \"\"\"\n",
    "    Defines a Hierarchical VAE (HVAE) with two latent layers to capture hierarchical representations.\n",
    "    \"\"\"\n",
    "    n_mels, time_frames = N_MELS, TIME_FRAMES\n",
    "    latent_dim_1 = 64  # Higher-level features\n",
    "    latent_dim_2 = 32  # Lower-level details\n",
    "\n",
    "    # ---- ENCODER ---- #\n",
    "    encoder_input = Input(shape=(n_mels, time_frames))\n",
    "    encoder_flatten = Flatten()(encoder_input)\n",
    "\n",
    "    # First latent layer (z1)\n",
    "    z1_mean = Dense(latent_dim_1, activation=\"linear\")(encoder_flatten)\n",
    "    z1_log_var = Dense(latent_dim_1, activation=\"linear\")(encoder_flatten)\n",
    "\n",
    "    def sampling(args):\n",
    "        mean, log_var = args\n",
    "        batch = K.shape(mean)[0]\n",
    "        dim = K.int_shape(mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return mean + K.exp(0.5 * K.clip(log_var, -10, 10)) * epsilon  # Clip to prevent NaNs\n",
    "\n",
    "    z1 = Lambda(sampling, output_shape=(latent_dim_1,))([z1_mean, z1_log_var])\n",
    "\n",
    "    # Second latent layer (z2) based on z1\n",
    "    z2_mean = Dense(latent_dim_2, activation=\"linear\")(z1)\n",
    "    z2_log_var = Dense(latent_dim_2, activation=\"linear\")(z1)\n",
    "    z2 = Lambda(sampling, output_shape=(latent_dim_2,))([z2_mean, z2_log_var])\n",
    "\n",
    "    encoder = Model(encoder_input, [z1_mean, z1_log_var, z2_mean, z2_log_var, z1, z2], name=\"encoder\")\n",
    "\n",
    "    # ---- DECODER ---- #\n",
    "    decoder_input = Input(shape=(latent_dim_2,))\n",
    "    decoder_dense = Dense(n_mels * time_frames, activation=\"sigmoid\")(decoder_input)\n",
    "    decoder_reshaped = Reshape((n_mels, time_frames))(decoder_dense)\n",
    "    decoder = Model(decoder_input, decoder_reshaped, name=\"decoder\")\n",
    "\n",
    "    # ---- LOSS FUNCTION ---- #\n",
    "    def hvae_loss_function(args):\n",
    "        \"\"\"\n",
    "        Computes the HVAE loss with reconstruction loss and KL divergence.\n",
    "        \"\"\"\n",
    "        y_true, y_pred, z1_mean, z1_log_var, z2_mean, z2_log_var = args\n",
    "        \n",
    "        reconstruction_loss = K.mean(K.square(y_true - y_pred))\n",
    "\n",
    "        kl_loss_z1 = -0.5 * K.mean(1 + K.clip(z1_log_var, -10, 10) - K.square(z1_mean) - K.exp(K.clip(z1_log_var, -10, 10)))   #(Ensure numerical stability)\n",
    "        kl_loss_z2 = -0.5 * K.mean(1 + K.clip(z2_log_var, -10, 10) - K.square(z2_mean) - K.exp(K.clip(z2_log_var, -10, 10)))\n",
    "\n",
    "        return reconstruction_loss + kl_loss_z1 + kl_loss_z2\n",
    "\n",
    "    # ---- FULL HVAE MODEL ---- #\n",
    "    hvae_input = Input(shape=(n_mels, time_frames))\n",
    "    hvae_target = Input(shape=(n_mels, time_frames))\n",
    "\n",
    "    encoder_output = encoder(hvae_input)\n",
    "    decoder_output = decoder(encoder_output[5])  # Use z2 for final reconstruction\n",
    "\n",
    "    # Add loss function to model\n",
    "    loss = Lambda(hvae_loss_function, output_shape=(1,), name='loss')(\n",
    "        [hvae_target, decoder_output, encoder_output[0], encoder_output[1], encoder_output[2], encoder_output[3]]\n",
    "    )\n",
    "    \n",
    "    hvae = Model(inputs=[hvae_input, hvae_target], outputs=[decoder_output, loss])\n",
    "    \n",
    "    return hvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc470c-0819-4222-a8ce-3075ad4f6282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "jazz_songs = np.array(jazz_df['gt_spectrogram'].tolist())\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "\n",
    "# Compile the HVAE\n",
    "hvae = create_hvae()\n",
    "hvae.compile(optimizer=Adam(learning_rate=0.001), loss=['mse', None])\n",
    "\n",
    "# Train the HVAE\n",
    "history_hvae = hvae.fit([jazz_songs, jazz_songs], [jazz_songs, np.zeros_like(jazz_songs)],\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(history_hvae.history['loss'])\n",
    "plt.title('Loss During Training (HVAE)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5411a0-5769-4ef8-8411-8d030cbb31cd",
   "metadata": {},
   "source": [
    "## Evaluation of HVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc55483-591f-4295-81de-8b1ec35d6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_spectrograms_hvae, _ = hvae.predict([jazz_songs, jazz_songs])\n",
    "jazz_df['pred_spectrogram_hvae'] = pd.Series(pred_spectrograms_hvae.tolist())\n",
    "\n",
    "jazz_df['cosine_similarity_hvae'] = compute_cosine_similarity(jazz_df, 'gt_spectrogram', 'pred_spectrogram_hvae')\n",
    "\n",
    "# Find best and worst reconstructions for HVAE\n",
    "worst_idx_hvae = jazz_df['cosine_similarity_hvae'].idxmin()\n",
    "best_idx_hvae = jazz_df['cosine_similarity_hvae'].idxmax()\n",
    "\n",
    "print(f\"Worst HVAE Reconstruction Index: {worst_idx_hvae}\")\n",
    "print(f\"Best HVAE Reconstruction Index: {best_idx_hvae}\")\n",
    "\n",
    "print(jazz_df['cosine_similarity_hvae'].describe())  # Summarize similarity scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0741103-54b3-434d-a07b-b9d13239053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_song_audios_hvae_and_save(jazz_df, sample_n, output_filename=None):\n",
    "    try:\n",
    "        pred_audio_hvae = reconstruct_audio(\n",
    "            jazz_df.iloc[sample_n]['pred_spectrogram_hvae'],\n",
    "            sr=22050, n_mels=128, min_max=jazz_df.iloc[sample_n]['min_max']\n",
    "        )\n",
    "\n",
    "        original_audio = jazz_df.iloc[sample_n]['gt_audio']\n",
    "\n",
    "        pred_audio_int16 = (pred_audio_hvae / np.max(np.abs(pred_audio_hvae)) * 32767).astype(np.int16)\n",
    "\n",
    "        # Save if output filename is provided\n",
    "        if output_filename is not None:\n",
    "            try:\n",
    "                sf.write(output_filename, pred_audio_int16, 22050)\n",
    "                print(f\"Saved audio to: {output_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving {output_filename}: {e}\")\n",
    "\n",
    "        # Normalize for IPython Audio playback (IPython requires float32 in [-1,1])\n",
    "        pred_audio_ipd = pred_audio_hvae / (np.max(np.abs(pred_audio_hvae)) + 1e-7)\n",
    "\n",
    "        return ipd.Audio(original_audio, rate=22050), ipd.Audio(pred_audio_ipd, rate=22050)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "print(\"Worst:\")\n",
    "real_hvae, pred_hvae = compare_song_audios_hvae_and_save(jazz_df, worst_idx_hvae, output_filename=\"output/HVAE_worst.wav\")\n",
    "display(real_hvae)\n",
    "display(pred_hvae)\n",
    "\n",
    "print(\"Best:\")\n",
    "real_hvae, pred_hvae = compare_song_audios_hvae_and_save(jazz_df, best_idx_hvae, output_filename=\"output/HVAE_best.wav\")\n",
    "display(real_hvae)\n",
    "display(pred_hvae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c373433-17c9-40c1-99c0-16c1707ccdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hvae_spectrogram_comparison(jazz_df: pd.DataFrame, index: int):\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "    axes[0].imshow(jazz_df.iloc[index]['gt_spectrogram'], origin='lower', aspect='auto')\n",
    "    axes[0].set_title(\"Original Spectrogram\")\n",
    "\n",
    "    # HVAE  \n",
    "    axes[1].imshow(jazz_df.iloc[index]['pred_spectrogram_hvae'], origin='lower', aspect='auto')\n",
    "    axes[1].set_title(\"HVAE Reconstructed Spectrogram\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_hvae_spectrogram_comparison(jazz_df, worst_idx_hvae)\n",
    "plot_hvae_spectrogram_comparison(jazz_df, best_idx_hvae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cfd41d-40d0-486c-870a-f1e69b05b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.hist(jazz_df['cosine_similarity'], bins=50, alpha=0.6, label=\"Vanilla VAE\", color=\"blue\")\n",
    "plt.hist(jazz_df['cosine_similarity_hvae'], bins=50, alpha=0.6, label=\"HVAE\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Cosine Similarity Distribution: Vanilla VAE vs HVAE\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880b02a-0899-4a84-a7e3-7603ba383d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vanilla VAE Best Similarity: {jazz_df['cosine_similarity'].max():.4f}\")\n",
    "print(f\"HVAE Best Similarity: {jazz_df['cosine_similarity_hvae'].max():.4f}\")\n",
    "\n",
    "print(f\"Vanilla VAE Worst Similarity: {jazz_df['cosine_similarity'].min():.4f}\")\n",
    "print(f\"HVAE Worst Similarity: {jazz_df['cosine_similarity_hvae'].min():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c0511-9adc-4b54-a7e9-686ddc6df74b",
   "metadata": {},
   "source": [
    "The random sampling with added noise from the latent space lead to poor results.\n",
    "\n",
    "If the reconstruction quality is poor, then the model is fundamentally not learning meaningful representations. This means the issue is not in the sampling process but in the training and model architecture itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f50c3a-645f-4498-9c15-330ed80015c2",
   "metadata": {},
   "source": [
    "# VAE with Simple Waveforms\n",
    "\n",
    "For one final approach, we can maintain a simpler architecture but adopt a new data representation which analyzes the waveform itself instead of a spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f13b03-c088-4ee1-a9a2-891d88205948",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_lengths = [len(audio) for audio in jazz_df['gt_audio']]\n",
    "print(f\"Max length: {max(audio_lengths)}, Min length: {min(audio_lengths)}, Median length: {np.median(audio_lengths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff550b-1c3c-42ce-b567-055667f22d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Input, Conv1D, Conv1DTranspose, Dense, Lambda, GlobalAveragePooling1D, BatchNormalization, Reshape, UpSampling1D\n",
    "\n",
    "# ---- HYPERPARAMETERS ---- #\n",
    "sequence_length = (1280 //8) * 8 # Number of waveform samples, making sure it can be devided by 8\n",
    "latent_dim = 8  \n",
    "\n",
    "# ---- MODEL ARCHITECTURE ---- #\n",
    "def get_encoder(sequence_length, latent_dim):\n",
    "    \"\"\"\n",
    "    Encoder for 1D waveform processing using Conv1D layers.\n",
    "    \"\"\"\n",
    "    encoder_input = Input(shape=(sequence_length, 1))\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\n",
    "    x = Conv1D(128, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = Conv1D(256, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    # Flattening lead to milions paramters, instead we are trying Global Average Pooling \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    z_mean = Dense(latent_dim, activation=\"linear\")(x)\n",
    "    z_mean = BatchNormalization()(z_mean)  # Helps latent stability\n",
    "    z_log_var = Dense(latent_dim, activation=\"linear\")(x)\n",
    "    \n",
    "    def sampling(args):\n",
    "        mean, log_var = args\n",
    "        epsilon = K.random_normal(shape=K.shape(mean), mean=0.0, stddev=0.1)  \n",
    "\n",
    "        return mean + K.exp(0.5 * K.clip(log_var, -10, 10)) * epsilon  # Clipping for numerical stability\n",
    "\n",
    "    z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "    encoder = Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    return encoder\n",
    "\n",
    "def get_decoder(sequence_length, latent_dim):\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "    target_shape = (sequence_length // 8, 256)  # Adjust based on encoder downsampling\n",
    "    x = Dense(np.prod(target_shape), activation=\"relu\")(decoder_input)\n",
    "    x = Reshape(target_shape)(x)  # (batch, time_steps//8, 128)\n",
    "\n",
    "    x = Conv1DTranspose(256, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = Conv1DTranspose(128, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = Conv1DTranspose(64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    # Final reconstruction layer\n",
    "    x = Conv1DTranspose(1, kernel_size=3, strides=1, activation=\"tanh\", padding=\"same\")(x)\n",
    "    \n",
    "    decoder = Model(decoder_input, x, name=\"decoder\")\n",
    "    return decoder\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, sequence_length, latent_dim, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sequence_length = sequence_length\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def vae_loss_function(self, y_true, y_pred, z_mean, z_log_var):\n",
    "        y_true = K.reshape(y_true, K.shape(y_pred))\n",
    "\n",
    "        reconstruction_loss = K.mean(K.square(y_true - y_pred))\n",
    "\n",
    "        beta = 1 \n",
    "        kl_loss = -0.5 * K.mean(1 + K.clip(z_log_var, -10, 10) - K.square(z_mean) - K.exp(K.clip(z_log_var, -10, 10)))\n",
    "\n",
    "        return reconstruction_loss + beta * kl_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstructed = self.decoder(z)\n",
    "            loss = self.vae_loss_function(data, reconstructed, z_mean, z_log_var)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Defines how the VAE processes input during inference.\"\"\"\n",
    "        z_mean, z_log_var, z = self.encoder(inputs) \n",
    "        reconstructed = self.decoder(z)  \n",
    "        return reconstructed  \n",
    "\n",
    "def create_vae(sequence_length, latent_dim):\n",
    "    encoder = get_encoder(sequence_length, latent_dim)\n",
    "    decoder = get_decoder(sequence_length, latent_dim)\n",
    "    vae = VAE(encoder, decoder, sequence_length, latent_dim)\n",
    "    return vae, encoder, decoder\n",
    "\n",
    "# ---- LOAD DATA ---- #\n",
    "def split_audio_into_chunks(waveform, chunk_size, stride):\n",
    "    \"\"\"Splits long waveforms into overlapping chunks.\"\"\"\n",
    "    num_chunks = max(1, (len(waveform) - chunk_size) // stride + 1)\n",
    "    chunks = [waveform[i * stride: i * stride + chunk_size] for i in range(num_chunks)]\n",
    "    return chunks\n",
    "\n",
    "def normalize_audio(waveform):\n",
    "    \"\"\"Normalize waveform between -1 and 1.\"\"\"\n",
    "    return waveform / np.max(np.abs(waveform)) \n",
    "\n",
    "jazz_waveforms = []\n",
    "for w in jazz_df['gt_audio']:\n",
    "    normalized_w = normalize_audio(w) \n",
    "    jazz_waveforms.extend(split_audio_into_chunks(normalized_w, chunk_size=sequence_length, stride=sequence_length*2))\n",
    "\n",
    "jazz_waveforms = np.array(jazz_waveforms)\n",
    "jazz_waveforms = np.expand_dims(jazz_waveforms, axis=-1)  # Shape: (batch_size, time_steps, 1)\n",
    "\n",
    "print(f\"Data input shape: {jazz_waveforms.shape}\") \n",
    "\n",
    "# ---- TRAIN ---- #\n",
    "# Instantiate the VAE model\n",
    "vae, encoder, decoder = create_vae(sequence_length, latent_dim)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0))\n",
    "\n",
    "history = vae.fit(jazz_waveforms, epochs=10, batch_size=100)  \n",
    "\n",
    "# ---- VISUALIZATION ---- #\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss During Training (VAE - Waveform)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d89d31-2a6d-44ec-b51f-f6b9505b92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "z_means, z_log_vars, z_samples = encoder.predict(jazz_waveforms)\n",
    "print(\"Mean of latent variables (z):\", np.mean(z_samples, axis=0))  # Should not be all zeros\n",
    "print(\"Variance of latent variables:\", np.var(z_samples, axis=0))  # Should not be too small\n",
    "\n",
    "z_means, _, z_samples = encoder.predict(jazz_waveforms)\n",
    "plt.figure()\n",
    "sns.histplot(z_samples.flatten(), bins=50, kde=True)\n",
    "plt.title(\"Latent Space Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da06c5b-c001-4e9a-84bd-04e4ec64a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- EVALUATION ---- #\n",
    "def compute_mse(df, col1, col2):\n",
    "    # to use in addition to cosine similarity\n",
    "    errors = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        waveform1 = np.array(row[col1])\n",
    "        waveform2 = np.array(row[col2])\n",
    "        \n",
    "        if waveform1.size == 0 or waveform2.size == 0:\n",
    "            errors.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        length_diff = len(waveform1) - len(waveform2)\n",
    "        if abs(length_diff) > 500:  # If difference is too large, just skip it\n",
    "            errors.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        # Pad the shorter waveform if needed\n",
    "        if length_diff > 0:  # waveform1 is longer\n",
    "            waveform2 = np.pad(waveform2, (0, length_diff), mode='constant')\n",
    "        elif length_diff < 0:  # waveform2 is longer\n",
    "            waveform1 = np.pad(waveform1, (0, -length_diff), mode='constant')\n",
    "\n",
    "        errors.append(np.mean(np.square(waveform1 - waveform2)))\n",
    "    \n",
    "    return np.array(errors)\n",
    "\n",
    "pred_waveforms = vae.predict(jazz_waveforms)\n",
    "jazz_df['pred_waveform'] = pd.Series(pred_waveforms.tolist())\n",
    "jazz_df['cosine_similarity'] = compute_cosine_similarity(jazz_df, 'gt_audio', 'pred_waveform')\n",
    "\n",
    "if jazz_df['cosine_similarity'].isna().all():\n",
    "    print(\"All cosine similarities are NaN!\")\n",
    "    worst_idx, best_idx = None, None\n",
    "else:\n",
    "    worst_idx = jazz_df['cosine_similarity'].idxmin(skipna=True)\n",
    "    best_idx = jazz_df['cosine_similarity'].idxmax(skipna=True)\n",
    "\n",
    "jazz_df['mse_loss'] = compute_mse(jazz_df, 'gt_audio', 'pred_waveform')\n",
    "\n",
    "# Identify best and worst samples based on MSE\n",
    "print(\"using MSE:\", jazz_df['mse_loss'].idxmax(), \"\\nusing cosine:\", worst_idx)  # Worst = highest error\n",
    "print(\"using MSE:\", jazz_df['mse_loss'].idxmin(), \"\\nusing cosine:\", best_idx)   # Best = lowest error\n",
    "\n",
    "# Compare Generated Audio\n",
    "def compare_audio(jazz_df, idx):\n",
    "    pred_audio = jazz_df.iloc[idx]['pred_waveform']\n",
    "    real_audio = jazz_df.iloc[idx]['gt_audio']\n",
    "    return ipd.Audio(real_audio, rate=22050), ipd.Audio(pred_audio, rate=22050)\n",
    "\n",
    "print(\"Worst:\")\n",
    "display(*compare_audio(jazz_df, worst_idx))\n",
    "\n",
    "print(\"Best:\")\n",
    "display(*compare_audio(jazz_df, best_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864bac4-5ff8-4446-9b19-edd172ae76a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d78c6-80e8-4056-8906-9534ececf0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff5e8a-3616-437a-b9f7-975b527d0bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8d2b5-25b6-4cd3-842e-ff7d3b44752d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
