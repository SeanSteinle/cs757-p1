{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e709312-3848-4a92-a39f-3c12fd97eb19",
   "metadata": {},
   "source": [
    "# Improving Our VAE\n",
    "*Kiya Aminfar, Sean Steinle*\n",
    "\n",
    "This notebook is our attempt to improve upon `./simple_vae.ipynb`, where we trained a simple but ineffective variational autoencoder to generate music. In this notebook, we try three techniques to improve our autoencoder: more epochs, more data, and more layers.\n",
    "\n",
    "## Table of Contents\n",
    "1. [More Epochs](#More-Epochs)\n",
    "2. [More Data](#More-Data)\n",
    "3. [More Layers](#More-Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c383e7ab-09b6-4ef5-a06a-49f5a2c9cbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:22:49.813725: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-14 14:22:49.840199: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-14 14:22:50.087594: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-14 14:22:50.261332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741976570.410230   26290 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741976570.456347   26290 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 14:22:50.839672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import librosa, os #audio processing and file system parsing\n",
    "import librosa.display\n",
    "import numpy as np #math library\n",
    "import tensorflow as tf #for model building\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import pandas as pd #for data analysis / prep\n",
    "import IPython.display as ipd #for sound output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad25642-69de-4d67-8a2a-4d435b114bfb",
   "metadata": {},
   "source": [
    "## More Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6547c7-20a2-4507-9bc3-a8be5241c1fa",
   "metadata": {},
   "source": [
    "## More Data\n",
    "\n",
    "It looks like the [Free Music Archive](https://github.com/mdeff/fma) is the easiest way to get access to more music! They have a paper detailing all of their work, freely available metadata, and audio-only zips. Let's start with the smallest audio-only zip (`fma_small.zip`), which contains about 8,000 tracks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec813e8-f152-4bbf-8068-fa5701d9175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SAMPLE_RATE = 22050  # Standard sample rate for music processing\n",
    "N_MELS = 128         # Number of Mel filterbanks\n",
    "HOP_LENGTH = 512     # Hop length for STFT\n",
    "N_FFT = 2048         # FFT window size\n",
    "DURATION = 5         # Duration of each audio clip in seconds\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_audio_to_mel(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to log scale (dB)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())\n",
    "    return mel_spec_norm\n",
    "\n",
    "def mel_to_audio(mel_spec, sr=22050, n_fft=2048, hop_length=512, n_mels=128, power=1.0):\n",
    "    \"\"\"\n",
    "    Convert a Mel spectrogram back to audio using the Griffin-Lim algorithm.\n",
    "    Args:\n",
    "        mel_spec: Mel spectrogram (shape: [n_mels, time_steps])\n",
    "        sr: Sample rate for the audio\n",
    "        n_fft: FFT size for Griffin-Lim\n",
    "        hop_length: Hop length for Griffin-Lim\n",
    "        n_mels: Number of Mel bins in the spectrogram\n",
    "        power: Exponent for the spectrogram\n",
    "    Returns:\n",
    "        Audio signal as a numpy array\n",
    "    \"\"\"\n",
    "    # Invert Mel to linear scale\n",
    "    mel_inverted = librosa.feature.inverse.mel_to_audio(mel_spec ** power, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    return mel_inverted\n",
    "\n",
    "def load_df_from_kaggle(music_dir: str):\n",
    "    \"\"\"This function loads a directory of songs and creates a simple dataframe with (genre,song,numpy) rows, based on the Kaggle dataset. We expect music_dir to be a 2-level directory with genre directories on the first level and .wav songs on the seocnd level.\"\"\"\n",
    "    music_dicts,bad_paths = [],[]\n",
    "    genres = os.listdir(music_dir)\n",
    "    for genre in genres:\n",
    "        try:\n",
    "            for song in os.listdir(music_dir+genre):\n",
    "                song_path = music_dir+genre+'/'+song\n",
    "                try:\n",
    "                    music_dicts.append({'genre': genre, 'song': song, 'numpy_representation': load_audio_to_mel(song_path)})\n",
    "                except Exception as e:\n",
    "                    print(f\"couldn't load: {song_path}, got: {e}\")\n",
    "                    bad_paths.append([song_path,e])\n",
    "        except Exception as e:\n",
    "            print(f\"couldn't process the {genre} directory, got: {e}\")\n",
    "    return pd.DataFrame(music_dicts)\n",
    "\n",
    "def load_df_from_fma(music_dir: str, max_songs: int=8000):\n",
    "    \"\"\"This function loads a directory of songs and creates a simple dataframe with (song,numpy) rows, based on the FMA dataset. We expect music_dir to be a single-level directory with just songs.\"\"\"\n",
    "    music_dicts,bad_paths = [],[]\n",
    "    for i,song in enumerate(os.listdir(music_dir)):\n",
    "        song_path = music_dir+song\n",
    "        try:\n",
    "            audio = load_audio_to_mel(song_path)\n",
    "            if np.isnan(audio).sum() > 0: \n",
    "                raise Exception(\"got nan in audio\")\n",
    "            music_dicts.append({'song': song, 'numpy_representation': audio})\n",
    "        except Exception as e:\n",
    "            print(f\"couldn't load: {song_path}, got: {e}\")\n",
    "            bad_paths.append([song_path,e])\n",
    "        if i >= max_songs:\n",
    "            break\n",
    "    return pd.DataFrame(music_dicts)\n",
    "\n",
    "def create_tf_dataset(music_df):\n",
    "    \"\"\"Converts music_df's numpy column to a TensorFlow dataset.\"\"\"\n",
    "    assert \"numpy_representation\" in music_df.columns\n",
    "    numpy_representations = np.array(music_df[\"numpy_representation\"].tolist(), dtype=np.float32)\n",
    "\n",
    "    # Ensure shape is (128, 216, 1) (Mel bins, Time steps, Channels)\n",
    "    numpy_representations = np.expand_dims(numpy_representations, -1)  # Add channel dimension\n",
    "\n",
    "    songs_dataset = tf.data.Dataset.from_tensor_slices(numpy_representations)\n",
    "    songs_dataset = songs_dataset.shuffle(len(numpy_representations)).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return songs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cc0a5-9c53-4ac0-870a-29ae636268e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = load_df_from_kaggle(\"../data/genres_original/\")\n",
    "songs_dataset = create_tf_dataset(music_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92487902-3516-4851-8304-92840a399877",
   "metadata": {},
   "source": [
    "## More Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323b23c-1686-4138-9cb1-b9f302b1110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "LATENT_DIM = 64  # Latent space dimension\n",
    "INPUT_SHAPE = (128, 216)  # (Time steps, Mel bins)\n",
    "\n",
    "# Reparameterization Trick\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick: z = mu + exp(log_var / 2) * epsilon\"\"\"\n",
    "    mu, log_var = args\n",
    "    epsilon = tf.keras.backend.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var * 0.5) * epsilon\n",
    "\n",
    "# Encoder\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    mu = layers.Dense(latent_dim, name=\"latent_mu\")(x)\n",
    "    log_var = layers.Dense(latent_dim, name=\"latent_log_var\")(x)\n",
    "    z = layers.Lambda(sampling, name=\"latent_sample\")([mu, log_var])\n",
    "    \n",
    "    return Model(inputs, [mu, log_var, z], name=\"Encoder\")\n",
    "\n",
    "# Decoder\n",
    "def build_decoder(latent_dim, output_shape):\n",
    "    decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = layers.Dense(output_shape[0] // 8 * 128, activation=\"relu\")(decoder_inputs)\n",
    "    x = layers.Reshape((output_shape[0] // 8, 128))(x)  # Adjust size\n",
    "    \n",
    "    x = layers.Conv1DTranspose(128, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1DTranspose(64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1DTranspose(32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    outputs = layers.Conv1DTranspose(output_shape[1], kernel_size=3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "    return Model(decoder_inputs, outputs, name=\"Decoder\")\n",
    "\n",
    "# VAE Model\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def compute_loss(self, inputs):\n",
    "        \"\"\"Computes the VAE loss function.\"\"\"\n",
    "        mu, log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        recon_loss = tf.reduce_mean(tf.keras.losses.mse(inputs, reconstructed))\n",
    "\n",
    "        # KL Divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mu) - tf.exp(log_var))\n",
    "\n",
    "        return recon_loss + kl_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"Custom training step.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(data)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "def generate_samples(model, num_samples=10):\n",
    "    \"\"\"Convenience function for generating num_samples from model.\"\"\"\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_samples, LATENT_DIM)) # Sample random latent vectors from the prior (e.g., normal distribution)\n",
    "    generated_samples = model.decoder(random_latent_vectors) # Use the decoder to generate samples from the random latent vectors\n",
    "    generated_samples = [s.numpy().squeeze() for s in generated_samples] #convert to numpy, reshape\n",
    "    return generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918d6e9-357f-4663-b9c8-304bddd6405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de8132-fc25-4f4c-9f1d-69f9dffe079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7993b6-315e-44d0-888a-890dc0ccc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder and decoder\n",
    "encoder = build_encoder(INPUT_SHAPE, LATENT_DIM)\n",
    "decoder = build_decoder(LATENT_DIM, INPUT_SHAPE)\n",
    "\n",
    "# Create the VAE instance\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 20\n",
    "vae.fit(songs_dataset, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31a41f-fc19-4193-84e9-1da0a663415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generate_samples(vae, num_samples=5)\n",
    "mel_audio = mel_to_audio(generated_samples[0])\n",
    "ipd.Audio(mel_audio, rate=22050) #generated sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd851ac8-f1f7-480a-921b-8caf3107ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hm, same poor results. what if we just train with classical and if we train for longer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45315264-52e0-4619-bbcb-ea51ad7adb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = load_df_from_kaggle(\"../data/genres_original/\")\n",
    "classical_df = music_df[music_df['genre'] == 'classical']\n",
    "songs_dataset = create_tf_dataset(classical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666a376-7a9e-4313-a59b-17c265975c91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the encoder and decoder\n",
    "encoder = build_encoder(INPUT_SHAPE, LATENT_DIM)\n",
    "decoder = build_decoder(LATENT_DIM, INPUT_SHAPE)\n",
    "\n",
    "# Create the VAE instance\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 100\n",
    "vae.fit(songs_dataset, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267cd30-893e-4485-af78-e3dd0aaeb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generate_samples(vae, num_samples=5)\n",
    "mel_audio = mel_to_audio(generated_samples[0])\n",
    "ipd.Audio(mel_audio, rate=22050) #generated sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a089e-9f17-4af4-b6b3-87e31a72b20a",
   "metadata": {},
   "source": [
    "## The FMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613dbc0-657c-48bf-8287-c7228319e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_track = \"../data/fma/small/000002.mp3\"\n",
    "audio, sr = librosa.load(mp3_track, sr=None)  # sr=None keeps the original sample rate\n",
    "print(\"Original Ground-Truth Audio:\")\n",
    "ipd.display(ipd.Audio(audio, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a66c1-4291-4281-a24b-643018929d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_audio_to_mel(mp3_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43aa00-ef1b-4669-8573-b0a35b3fd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fma_df = load_df_from_fma(\"../data/fma/small/\")\n",
    "songs_dataset = create_tf_dataset(fma_df)\n",
    "fma_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3039d9c-5abd-4979-baff-0d0e31086d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder and decoder\n",
    "encoder = build_encoder(INPUT_SHAPE, LATENT_DIM)\n",
    "decoder = build_decoder(LATENT_DIM, INPUT_SHAPE)\n",
    "\n",
    "# Create the VAE instance\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 5\n",
    "vae.fit(songs_dataset, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e239779-6c38-4e88-9b37-57df198aef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generate_samples(vae, num_samples=5)\n",
    "mel_audio = mel_to_audio(generated_samples[0])\n",
    "ipd.Audio(mel_audio, rate=22050) #generated sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs757-p1",
   "language": "python",
   "name": "cs757-p1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
